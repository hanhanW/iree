#translation = #iree_codegen.translation_info<None workgroup_size = [128, 2, 1] subgroup_size = 64, {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d2)>, affine_map<(d0, d1, d2) -> (d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>], loop_range = array<i64: 2048, 10240, 1280>}>
module {
  func.func @matmul_transpose_b_ukernel(%0: memref<2048x1280xf16, strided<[1280, 1], offset: ?>>, %16: memref<10240x1280xf16, strided<[1280, 1], offset: ?>>, %298: memref<2048x10240xf32, strided<[10240, 1], offset: ?>>) attributes {translation_info = #translation} {
    %workgroup_id_0 = stream.dispatch.workgroup.id[0] : index
    %workgroup_id_1 = stream.dispatch.workgroup.id[1] : index
    %thread_id_x = gpu.thread_id  x
    %thread_id_y = gpu.thread_id  y
    %thread_id_z = gpu.thread_id  z
    %alloc = memref.alloc() : memref<64x36xf16, #gpu.address_space<workgroup>>
    %alloc_0 = memref.alloc() : memref<64x36xf16, #gpu.address_space<workgroup>>
    %cst = arith.constant dense<0.000000e+00> : vector<4xf32>
    %cst_1 = arith.constant dense<0.000000e+00> : vector<4xf32>
    %cst_2 = arith.constant dense<0.000000e+00> : vector<4xf32>
    %cst_3 = arith.constant dense<0.000000e+00> : vector<4xf32>
    %c0 = arith.constant 0 : index
    %c32 = arith.constant 32 : index
    %1 = arith.muli %thread_id_y, %c32 : index
    %c64 = arith.constant 64 : index
    %2 = arith.muli %thread_id_z, %c64 : index
    %c64_4 = arith.constant 64 : index
    %c1 = arith.constant 1 : index
    %c160 = arith.constant 160 : index
    %c32_5 = arith.constant 32 : index
    %3 = arith.muli %workgroup_id_1, %c32_5 : index
    %4 = arith.addi %3, %workgroup_id_0 : index
    %5 = arith.divsi %4, %c160 : index
    %6 = arith.muli %5, %c64_4 : index
    %c1_6 = arith.constant 1 : index
    %c4 = arith.constant 4 : index
    %7 = arith.divsi %thread_id_x, %c4 : index
    %8 = arith.addi %7, %6 : index
    %9 = arith.addi %8, %2 : index
    %10 = arith.addi %9, %1 : index
    %c8 = arith.constant 8 : index
    %c4_7 = arith.constant 4 : index
    %11 = arith.remsi %thread_id_x, %c4_7 : index
    %12 = arith.muli %11, %c8 : index
    %c32_8 = arith.constant 32 : index
    %c0_9 = arith.constant 0 : index
    %13 = arith.muli %c0_9, %c32_8 : index
    %14 = arith.addi %13, %12 : index
    %15 = vector.load %0[%10, %14] : memref<2048x1280xf16, strided<[1280, 1], offset: ?>>, vector<8xf16>
    %c0_10 = arith.constant 0 : index
    %c32_11 = arith.constant 32 : index
    %17 = arith.muli %thread_id_y, %c32_11 : index
    %c64_12 = arith.constant 64 : index
    %18 = arith.muli %thread_id_z, %c64_12 : index
    %c64_13 = arith.constant 64 : index
    %c32_14 = arith.constant 32 : index
    %19 = arith.muli %workgroup_id_1, %c32_14 : index
    %20 = arith.addi %19, %workgroup_id_0 : index
    %c160_15 = arith.constant 160 : index
    %21 = arith.remsi %20, %c160_15 : index
    %22 = arith.muli %21, %c64_13 : index
    %c1_16 = arith.constant 1 : index
    %c4_17 = arith.constant 4 : index
    %23 = arith.divsi %thread_id_x, %c4_17 : index
    %24 = arith.addi %23, %22 : index
    %25 = arith.addi %24, %18 : index
    %26 = arith.addi %25, %17 : index
    %c8_18 = arith.constant 8 : index
    %c4_19 = arith.constant 4 : index
    %27 = arith.remsi %thread_id_x, %c4_19 : index
    %28 = arith.muli %27, %c8_18 : index
    %c32_20 = arith.constant 32 : index
    %c0_21 = arith.constant 0 : index
    %29 = arith.muli %c0_21, %c32_20 : index
    %30 = arith.addi %29, %28 : index
    %31 = vector.load %16[%26, %30] : memref<10240x1280xf16, strided<[1280, 1], offset: ?>>, vector<8xf16>
    %c0_i32 = arith.constant 0 : i32
    %c32_i32 = arith.constant 32 : i32
    %c2_i32 = arith.constant 2 : i32
    llvm.call_intrinsic "llvm.amdgcn.sched.group.barrier"(%c32_i32, %c2_i32, %c0_i32) : (i32, i32, i32) -> ()
    %c7_i32 = arith.constant 7 : i32
    llvm.call_intrinsic "llvm.amdgcn.sched.barrier"(%c7_i32) : (i32) -> ()
    %c32_22 = arith.constant 32 : index
    %32 = arith.muli %thread_id_y, %c32_22 : index
    %c64_23 = arith.constant 64 : index
    %33 = arith.muli %thread_id_z, %c64_23 : index
    %c64_24 = arith.constant 64 : index
    %c1_25 = arith.constant 1 : index
    %c160_26 = arith.constant 160 : index
    %c0_27 = arith.constant 0 : index
    %34 = arith.divsi %c0_27, %c160_26 : index
    %35 = arith.muli %34, %c64_24 : index
    %c1_28 = arith.constant 1 : index
    %c4_29 = arith.constant 4 : index
    %36 = arith.divsi %thread_id_x, %c4_29 : index
    %37 = arith.addi %36, %35 : index
    %38 = arith.addi %37, %33 : index
    %39 = arith.addi %38, %32 : index
    %c8_30 = arith.constant 8 : index
    %c4_31 = arith.constant 4 : index
    %40 = arith.remsi %thread_id_x, %c4_31 : index
    %41 = arith.muli %40, %c8_30 : index
    vector.store %15, %alloc_0[%39, %41] : memref<64x36xf16, #gpu.address_space<workgroup>>, vector<8xf16>
    %c32_32 = arith.constant 32 : index
    %42 = arith.muli %thread_id_y, %c32_32 : index
    %c64_33 = arith.constant 64 : index
    %43 = arith.muli %thread_id_z, %c64_33 : index
    %c64_34 = arith.constant 64 : index
    %c0_35 = arith.constant 0 : index
    %c160_36 = arith.constant 160 : index
    %44 = arith.remsi %c0_35, %c160_36 : index
    %45 = arith.muli %44, %c64_34 : index
    %c1_37 = arith.constant 1 : index
    %c4_38 = arith.constant 4 : index
    %46 = arith.divsi %thread_id_x, %c4_38 : index
    %47 = arith.addi %46, %45 : index
    %48 = arith.addi %47, %43 : index
    %49 = arith.addi %48, %42 : index
    %c8_39 = arith.constant 8 : index
    %c4_40 = arith.constant 4 : index
    %50 = arith.remsi %thread_id_x, %c4_40 : index
    %51 = arith.muli %50, %c8_39 : index
    vector.store %31, %alloc[%49, %51] : memref<64x36xf16, #gpu.address_space<workgroup>>, vector<8xf16>
    %c32_41 = arith.constant 32 : index
    %52 = arith.muli %thread_id_y, %c32_41 : index
    %c64_42 = arith.constant 64 : index
    %53 = arith.muli %thread_id_z, %c64_42 : index
    %c64_43 = arith.constant 64 : index
    %c1_44 = arith.constant 1 : index
    %c160_45 = arith.constant 160 : index
    %c32_46 = arith.constant 32 : index
    %54 = arith.muli %workgroup_id_1, %c32_46 : index
    %55 = arith.addi %54, %workgroup_id_0 : index
    %56 = arith.divsi %55, %c160_45 : index
    %57 = arith.muli %56, %c64_43 : index
    %c1_47 = arith.constant 1 : index
    %c4_48 = arith.constant 4 : index
    %58 = arith.divsi %thread_id_x, %c4_48 : index
    %59 = arith.addi %58, %57 : index
    %60 = arith.addi %59, %53 : index
    %61 = arith.addi %60, %52 : index
    %c8_49 = arith.constant 8 : index
    %c4_50 = arith.constant 4 : index
    %62 = arith.remsi %thread_id_x, %c4_50 : index
    %63 = arith.muli %62, %c8_49 : index
    %c32_51 = arith.constant 32 : index
    %c1_52 = arith.constant 1 : index
    %64 = arith.muli %c1_52, %c32_51 : index
    %65 = arith.addi %64, %63 : index
    %66 = vector.load %0[%61, %65] : memref<2048x1280xf16, strided<[1280, 1], offset: ?>>, vector<8xf16>
    %c32_53 = arith.constant 32 : index
    %67 = arith.muli %thread_id_y, %c32_53 : index
    %c64_54 = arith.constant 64 : index
    %68 = arith.muli %thread_id_z, %c64_54 : index
    %c64_55 = arith.constant 64 : index
    %c32_56 = arith.constant 32 : index
    %69 = arith.muli %workgroup_id_1, %c32_56 : index
    %70 = arith.addi %69, %workgroup_id_0 : index
    %c160_57 = arith.constant 160 : index
    %71 = arith.remsi %70, %c160_57 : index
    %72 = arith.muli %71, %c64_55 : index
    %c1_58 = arith.constant 1 : index
    %c4_59 = arith.constant 4 : index
    %73 = arith.divsi %thread_id_x, %c4_59 : index
    %74 = arith.addi %73, %72 : index
    %75 = arith.addi %74, %68 : index
    %76 = arith.addi %75, %67 : index
    %c8_60 = arith.constant 8 : index
    %c4_61 = arith.constant 4 : index
    %77 = arith.remsi %thread_id_x, %c4_61 : index
    %78 = arith.muli %77, %c8_60 : index
    %c32_62 = arith.constant 32 : index
    %c1_63 = arith.constant 1 : index
    %79 = arith.muli %c1_63, %c32_62 : index
    %80 = arith.addi %79, %78 : index
    %81 = vector.load %16[%76, %80] : memref<10240x1280xf16, strided<[1280, 1], offset: ?>>, vector<8xf16>
    %c0_i32_64 = arith.constant 0 : i32
    %c512_i32 = arith.constant 512 : i32
    %c2_i32_65 = arith.constant 2 : i32
    llvm.call_intrinsic "llvm.amdgcn.sched.group.barrier"(%c512_i32, %c2_i32_65, %c0_i32_64) : (i32, i32, i32) -> ()
    %c32_i32_66 = arith.constant 32 : i32
    %c2_i32_67 = arith.constant 2 : i32
    llvm.call_intrinsic "llvm.amdgcn.sched.group.barrier"(%c32_i32_66, %c2_i32_67, %c0_i32_64) : (i32, i32, i32) -> ()
    %c7_i32_68 = arith.constant 7 : i32
    llvm.call_intrinsic "llvm.amdgcn.sched.barrier"(%c7_i32_68) : (i32) -> ()
    amdgpu.lds_barrier
    amdgpu.lds_barrier
    %c16 = arith.constant 16 : index
    %c32_69 = arith.constant 32 : index
    %c1_70 = arith.constant 1 : index
    %c64_71 = arith.constant 64 : index
    %82 = arith.divsi %thread_id_x, %c64_71 : index
    %83 = arith.muli %82, %c32_69 : index
    %c64_72 = arith.constant 64 : index
    %c1_73 = arith.constant 1 : index
    %c160_74 = arith.constant 160 : index
    %c0_75 = arith.constant 0 : index
    %84 = arith.divsi %c0_75, %c160_74 : index
    %85 = arith.muli %84, %c64_72 : index
    %c16_76 = arith.constant 16 : index
    %86 = arith.remsi %thread_id_x, %c16_76 : index
    %87 = arith.addi %86, %85 : index
    %88 = arith.addi %87, %83 : index
    %89 = arith.addi %88, %c16 : index
    %c16_77 = arith.constant 16 : index
    %c4_78 = arith.constant 4 : index
    %c1_79 = arith.constant 1 : index
    %c16_80 = arith.constant 16 : index
    %c64_81 = arith.constant 64 : index
    %90 = arith.remsi %thread_id_x, %c64_81 : index
    %91 = arith.divsi %90, %c16_80 : index
    %92 = arith.muli %91, %c4_78 : index
    %93 = arith.addi %92, %c16_77 : index
    %94 = vector.load %alloc_0[%89, %93] : memref<64x36xf16, #gpu.address_space<workgroup>>, vector<4xf16>
    %c16_82 = arith.constant 16 : index
    %c32_83 = arith.constant 32 : index
    %95 = arith.muli %thread_id_y, %c32_83 : index
    %c64_84 = arith.constant 64 : index
    %c0_85 = arith.constant 0 : index
    %c160_86 = arith.constant 160 : index
    %96 = arith.remsi %c0_85, %c160_86 : index
    %97 = arith.muli %96, %c64_84 : index
    %c16_87 = arith.constant 16 : index
    %98 = arith.remsi %thread_id_x, %c16_87 : index
    %99 = arith.addi %98, %97 : index
    %100 = arith.addi %99, %95 : index
    %101 = arith.addi %100, %c16_82 : index
    %c16_88 = arith.constant 16 : index
    %c4_89 = arith.constant 4 : index
    %c1_90 = arith.constant 1 : index
    %c16_91 = arith.constant 16 : index
    %c64_92 = arith.constant 64 : index
    %102 = arith.remsi %thread_id_x, %c64_92 : index
    %103 = arith.divsi %102, %c16_91 : index
    %104 = arith.muli %103, %c4_89 : index
    %105 = arith.addi %104, %c16_88 : index
    %106 = vector.load %alloc[%101, %105] : memref<64x36xf16, #gpu.address_space<workgroup>>, vector<4xf16>
    %c0_i32_93 = arith.constant 0 : i32
    %c256_i32 = arith.constant 256 : i32
    %c2_i32_94 = arith.constant 2 : i32
    llvm.call_intrinsic "llvm.amdgcn.sched.group.barrier"(%c256_i32, %c2_i32_94, %c0_i32_93) : (i32, i32, i32) -> ()
    %c7_i32_95 = arith.constant 7 : i32
    llvm.call_intrinsic "llvm.amdgcn.sched.barrier"(%c7_i32_95) : (i32) -> ()
    %c16_96 = arith.constant 16 : index
    %c32_97 = arith.constant 32 : index
    %c1_98 = arith.constant 1 : index
    %c64_99 = arith.constant 64 : index
    %107 = arith.divsi %thread_id_x, %c64_99 : index
    %108 = arith.muli %107, %c32_97 : index
    %c64_100 = arith.constant 64 : index
    %c1_101 = arith.constant 1 : index
    %c160_102 = arith.constant 160 : index
    %c0_103 = arith.constant 0 : index
    %109 = arith.divsi %c0_103, %c160_102 : index
    %110 = arith.muli %109, %c64_100 : index
    %c16_104 = arith.constant 16 : index
    %111 = arith.remsi %thread_id_x, %c16_104 : index
    %112 = arith.addi %111, %110 : index
    %113 = arith.addi %112, %108 : index
    %114 = arith.addi %113, %c16_96 : index
    %c4_105 = arith.constant 4 : index
    %c1_106 = arith.constant 1 : index
    %c16_107 = arith.constant 16 : index
    %c64_108 = arith.constant 64 : index
    %115 = arith.remsi %thread_id_x, %c64_108 : index
    %116 = arith.divsi %115, %c16_107 : index
    %117 = arith.muli %116, %c4_105 : index
    %118 = vector.load %alloc_0[%114, %117] : memref<64x36xf16, #gpu.address_space<workgroup>>, vector<4xf16>
    %c16_109 = arith.constant 16 : index
    %c32_110 = arith.constant 32 : index
    %119 = arith.muli %thread_id_y, %c32_110 : index
    %c64_111 = arith.constant 64 : index
    %c0_112 = arith.constant 0 : index
    %c160_113 = arith.constant 160 : index
    %120 = arith.remsi %c0_112, %c160_113 : index
    %121 = arith.muli %120, %c64_111 : index
    %c16_114 = arith.constant 16 : index
    %122 = arith.remsi %thread_id_x, %c16_114 : index
    %123 = arith.addi %122, %121 : index
    %124 = arith.addi %123, %119 : index
    %125 = arith.addi %124, %c16_109 : index
    %c4_115 = arith.constant 4 : index
    %c1_116 = arith.constant 1 : index
    %c16_117 = arith.constant 16 : index
    %c64_118 = arith.constant 64 : index
    %126 = arith.remsi %thread_id_x, %c64_118 : index
    %127 = arith.divsi %126, %c16_117 : index
    %128 = arith.muli %127, %c4_115 : index
    %129 = vector.load %alloc[%125, %128] : memref<64x36xf16, #gpu.address_space<workgroup>>, vector<4xf16>
    %c0_i32_119 = arith.constant 0 : i32
    %c256_i32_120 = arith.constant 256 : i32
    %c2_i32_121 = arith.constant 2 : i32
    llvm.call_intrinsic "llvm.amdgcn.sched.group.barrier"(%c256_i32_120, %c2_i32_121, %c0_i32_119) : (i32, i32, i32) -> ()
    %c7_i32_122 = arith.constant 7 : i32
    llvm.call_intrinsic "llvm.amdgcn.sched.barrier"(%c7_i32_122) : (i32) -> ()
    %130 = amdgpu.mfma %118 * %129 + %cst_3 {blocks = 1 : i32, k = 16 : i32, m = 16 : i32, n = 16 : i32} blgp =  none : vector<4xf16>, vector<4xf16>, vector<4xf32>
    %c32_123 = arith.constant 32 : index
    %131 = arith.muli %thread_id_y, %c32_123 : index
    %c64_124 = arith.constant 64 : index
    %c0_125 = arith.constant 0 : index
    %c160_126 = arith.constant 160 : index
    %132 = arith.remsi %c0_125, %c160_126 : index
    %133 = arith.muli %132, %c64_124 : index
    %c16_127 = arith.constant 16 : index
    %134 = arith.remsi %thread_id_x, %c16_127 : index
    %135 = arith.addi %134, %133 : index
    %136 = arith.addi %135, %131 : index
    %c16_128 = arith.constant 16 : index
    %c4_129 = arith.constant 4 : index
    %c1_130 = arith.constant 1 : index
    %c16_131 = arith.constant 16 : index
    %c64_132 = arith.constant 64 : index
    %137 = arith.remsi %thread_id_x, %c64_132 : index
    %138 = arith.divsi %137, %c16_131 : index
    %139 = arith.muli %138, %c4_129 : index
    %140 = arith.addi %139, %c16_128 : index
    %141 = vector.load %alloc[%136, %140] : memref<64x36xf16, #gpu.address_space<workgroup>>, vector<4xf16>
    %c32_133 = arith.constant 32 : index
    %142 = arith.muli %thread_id_y, %c32_133 : index
    %c64_134 = arith.constant 64 : index
    %c0_135 = arith.constant 0 : index
    %c160_136 = arith.constant 160 : index
    %143 = arith.remsi %c0_135, %c160_136 : index
    %144 = arith.muli %143, %c64_134 : index
    %c16_137 = arith.constant 16 : index
    %145 = arith.remsi %thread_id_x, %c16_137 : index
    %146 = arith.addi %145, %144 : index
    %147 = arith.addi %146, %142 : index
    %c4_138 = arith.constant 4 : index
    %c1_139 = arith.constant 1 : index
    %c16_140 = arith.constant 16 : index
    %c64_141 = arith.constant 64 : index
    %148 = arith.remsi %thread_id_x, %c64_141 : index
    %149 = arith.divsi %148, %c16_140 : index
    %150 = arith.muli %149, %c4_138 : index
    %151 = vector.load %alloc[%147, %150] : memref<64x36xf16, #gpu.address_space<workgroup>>, vector<4xf16>
    %c0_i32_142 = arith.constant 0 : i32
    %c8_i32 = arith.constant 8 : i32
    %c1_i32 = arith.constant 1 : i32
    llvm.call_intrinsic "llvm.amdgcn.sched.group.barrier"(%c8_i32, %c1_i32, %c0_i32_142) : (i32, i32, i32) -> ()
    %c256_i32_143 = arith.constant 256 : i32
    %c2_i32_144 = arith.constant 2 : i32
    llvm.call_intrinsic "llvm.amdgcn.sched.group.barrier"(%c256_i32_143, %c2_i32_144, %c0_i32_142) : (i32, i32, i32) -> ()
    %c7_i32_145 = arith.constant 7 : i32
    llvm.call_intrinsic "llvm.amdgcn.sched.barrier"(%c7_i32_145) : (i32) -> ()
    %152 = amdgpu.mfma %118 * %151 + %cst_2 {blocks = 1 : i32, k = 16 : i32, m = 16 : i32, n = 16 : i32} blgp =  none : vector<4xf16>, vector<4xf16>, vector<4xf32>
    %c32_146 = arith.constant 32 : index
    %c1_147 = arith.constant 1 : index
    %c64_148 = arith.constant 64 : index
    %153 = arith.divsi %thread_id_x, %c64_148 : index
    %154 = arith.muli %153, %c32_146 : index
    %c64_149 = arith.constant 64 : index
    %c1_150 = arith.constant 1 : index
    %c160_151 = arith.constant 160 : index
    %c0_152 = arith.constant 0 : index
    %155 = arith.divsi %c0_152, %c160_151 : index
    %156 = arith.muli %155, %c64_149 : index
    %c16_153 = arith.constant 16 : index
    %157 = arith.remsi %thread_id_x, %c16_153 : index
    %158 = arith.addi %157, %156 : index
    %159 = arith.addi %158, %154 : index
    %c16_154 = arith.constant 16 : index
    %c4_155 = arith.constant 4 : index
    %c1_156 = arith.constant 1 : index
    %c16_157 = arith.constant 16 : index
    %c64_158 = arith.constant 64 : index
    %160 = arith.remsi %thread_id_x, %c64_158 : index
    %161 = arith.divsi %160, %c16_157 : index
    %162 = arith.muli %161, %c4_155 : index
    %163 = arith.addi %162, %c16_154 : index
    %164 = vector.load %alloc_0[%159, %163] : memref<64x36xf16, #gpu.address_space<workgroup>>, vector<4xf16>
    %c32_159 = arith.constant 32 : index
    %c1_160 = arith.constant 1 : index
    %c64_161 = arith.constant 64 : index
    %165 = arith.divsi %thread_id_x, %c64_161 : index
    %166 = arith.muli %165, %c32_159 : index
    %c64_162 = arith.constant 64 : index
    %c1_163 = arith.constant 1 : index
    %c160_164 = arith.constant 160 : index
    %c0_165 = arith.constant 0 : index
    %167 = arith.divsi %c0_165, %c160_164 : index
    %168 = arith.muli %167, %c64_162 : index
    %c16_166 = arith.constant 16 : index
    %169 = arith.remsi %thread_id_x, %c16_166 : index
    %170 = arith.addi %169, %168 : index
    %171 = arith.addi %170, %166 : index
    %c4_167 = arith.constant 4 : index
    %c1_168 = arith.constant 1 : index
    %c16_169 = arith.constant 16 : index
    %c64_170 = arith.constant 64 : index
    %172 = arith.remsi %thread_id_x, %c64_170 : index
    %173 = arith.divsi %172, %c16_169 : index
    %174 = arith.muli %173, %c4_167 : index
    %175 = vector.load %alloc_0[%171, %174] : memref<64x36xf16, #gpu.address_space<workgroup>>, vector<4xf16>
    %c0_i32_171 = arith.constant 0 : i32
    %c8_i32_172 = arith.constant 8 : i32
    %c1_i32_173 = arith.constant 1 : i32
    llvm.call_intrinsic "llvm.amdgcn.sched.group.barrier"(%c8_i32_172, %c1_i32_173, %c0_i32_171) : (i32, i32, i32) -> ()
    %c256_i32_174 = arith.constant 256 : i32
    %c2_i32_175 = arith.constant 2 : i32
    llvm.call_intrinsic "llvm.amdgcn.sched.group.barrier"(%c256_i32_174, %c2_i32_175, %c0_i32_171) : (i32, i32, i32) -> ()
    %c7_i32_176 = arith.constant 7 : i32
    llvm.call_intrinsic "llvm.amdgcn.sched.barrier"(%c7_i32_176) : (i32) -> ()
    %c2 = arith.constant 2 : index
    %c40 = arith.constant 40 : index
    %c1_177 = arith.constant 1 : index
    %176:15 = scf.for %arg3 = %c2 to %c40 step %c1_177 iter_args(%arg4 = %66, %arg5 = %81, %arg6 = %152, %arg7 = %130, %arg8 = %175, %arg9 = %164, %arg10 = %151, %arg11 = %141, %arg12 = %94, %arg13 = %129, %arg14 = %106, %arg15 = %cst, %arg16 = %cst_1, %arg17 = %cst_2, %arg18 = %cst_3) -> (vector<8xf16>, vector<8xf16>, vector<4xf32>, vector<4xf32>, vector<4xf16>, vector<4xf16>, vector<4xf16>, vector<4xf16>, vector<4xf16>, vector<4xf16>, vector<4xf16>, vector<4xf32>, vector<4xf32>, vector<4xf32>, vector<4xf32>) {
      %c32_374 = arith.constant 32 : index
      %416 = arith.muli %thread_id_y, %c32_374 : index
      %c64_375 = arith.constant 64 : index
      %417 = arith.muli %thread_id_z, %c64_375 : index
      %c64_376 = arith.constant 64 : index
      %c1_377 = arith.constant 1 : index
      %c160_378 = arith.constant 160 : index
      %c32_379 = arith.constant 32 : index
      %418 = arith.muli %workgroup_id_1, %c32_379 : index
      %419 = arith.addi %418, %workgroup_id_0 : index
      %420 = arith.divsi %419, %c160_378 : index
      %421 = arith.muli %420, %c64_376 : index
      %c1_380 = arith.constant 1 : index
      %c4_381 = arith.constant 4 : index
      %422 = arith.divsi %thread_id_x, %c4_381 : index
      %423 = arith.addi %422, %421 : index
      %424 = arith.addi %423, %417 : index
      %425 = arith.addi %424, %416 : index
      %c8_382 = arith.constant 8 : index
      %c4_383 = arith.constant 4 : index
      %426 = arith.remsi %thread_id_x, %c4_383 : index
      %427 = arith.muli %426, %c8_382 : index
      %c32_384 = arith.constant 32 : index
      %428 = arith.muli %arg3, %c32_384 : index
      %429 = arith.addi %428, %427 : index
      %430 = vector.load %0[%425, %429] : memref<2048x1280xf16, strided<[1280, 1], offset: ?>>, vector<8xf16>
      %c32_385 = arith.constant 32 : index
      %431 = arith.muli %thread_id_y, %c32_385 : index
      %c64_386 = arith.constant 64 : index
      %432 = arith.muli %thread_id_z, %c64_386 : index
      %c64_387 = arith.constant 64 : index
      %c32_388 = arith.constant 32 : index
      %433 = arith.muli %workgroup_id_1, %c32_388 : index
      %434 = arith.addi %433, %workgroup_id_0 : index
      %c160_389 = arith.constant 160 : index
      %435 = arith.remsi %434, %c160_389 : index
      %436 = arith.muli %435, %c64_387 : index
      %c1_390 = arith.constant 1 : index
      %c4_391 = arith.constant 4 : index
      %437 = arith.divsi %thread_id_x, %c4_391 : index
      %438 = arith.addi %437, %436 : index
      %439 = arith.addi %438, %432 : index
      %440 = arith.addi %439, %431 : index
      %c8_392 = arith.constant 8 : index
      %c4_393 = arith.constant 4 : index
      %441 = arith.remsi %thread_id_x, %c4_393 : index
      %442 = arith.muli %441, %c8_392 : index
      %c32_394 = arith.constant 32 : index
      %443 = arith.muli %arg3, %c32_394 : index
      %444 = arith.addi %443, %442 : index
      %445 = vector.load %16[%440, %444] : memref<10240x1280xf16, strided<[1280, 1], offset: ?>>, vector<8xf16>
      amdgpu.lds_barrier
      %c32_395 = arith.constant 32 : index
      %446 = arith.muli %thread_id_y, %c32_395 : index
      %c64_396 = arith.constant 64 : index
      %447 = arith.muli %thread_id_z, %c64_396 : index
      %c64_397 = arith.constant 64 : index
      %c1_398 = arith.constant 1 : index
      %c160_399 = arith.constant 160 : index
      %c0_400 = arith.constant 0 : index
      %448 = arith.divsi %c0_400, %c160_399 : index
      %449 = arith.muli %448, %c64_397 : index
      %c1_401 = arith.constant 1 : index
      %c4_402 = arith.constant 4 : index
      %450 = arith.divsi %thread_id_x, %c4_402 : index
      %451 = arith.addi %450, %449 : index
      %452 = arith.addi %451, %447 : index
      %453 = arith.addi %452, %446 : index
      %c8_403 = arith.constant 8 : index
      %c4_404 = arith.constant 4 : index
      %454 = arith.remsi %thread_id_x, %c4_404 : index
      %455 = arith.muli %454, %c8_403 : index
      vector.store %arg4, %alloc_0[%453, %455] : memref<64x36xf16, #gpu.address_space<workgroup>>, vector<8xf16>
      %c32_405 = arith.constant 32 : index
      %456 = arith.muli %thread_id_y, %c32_405 : index
      %c64_406 = arith.constant 64 : index
      %457 = arith.muli %thread_id_z, %c64_406 : index
      %c64_407 = arith.constant 64 : index
      %c0_408 = arith.constant 0 : index
      %c160_409 = arith.constant 160 : index
      %458 = arith.remsi %c0_408, %c160_409 : index
      %459 = arith.muli %458, %c64_407 : index
      %c1_410 = arith.constant 1 : index
      %c4_411 = arith.constant 4 : index
      %460 = arith.divsi %thread_id_x, %c4_411 : index
      %461 = arith.addi %460, %459 : index
      %462 = arith.addi %461, %457 : index
      %463 = arith.addi %462, %456 : index
      %c8_412 = arith.constant 8 : index
      %c4_413 = arith.constant 4 : index
      %464 = arith.remsi %thread_id_x, %c4_413 : index
      %465 = arith.muli %464, %c8_412 : index
      vector.store %arg5, %alloc[%463, %465] : memref<64x36xf16, #gpu.address_space<workgroup>>, vector<8xf16>
      %466 = amdgpu.mfma %arg12 * %arg14 + %arg7 {blocks = 1 : i32, k = 16 : i32, m = 16 : i32, n = 16 : i32} blgp =  none : vector<4xf16>, vector<4xf16>, vector<4xf32>
      %467 = amdgpu.mfma %arg8 * %arg13 + %arg16 {blocks = 1 : i32, k = 16 : i32, m = 16 : i32, n = 16 : i32} blgp =  none : vector<4xf16>, vector<4xf16>, vector<4xf32>
      %c0_i32_414 = arith.constant 0 : i32
      %c32_i32_415 = arith.constant 32 : i32
      %c2_i32_416 = arith.constant 2 : i32
      llvm.call_intrinsic "llvm.amdgcn.sched.group.barrier"(%c32_i32_415, %c2_i32_416, %c0_i32_414) : (i32, i32, i32) -> ()
      %c512_i32_417 = arith.constant 512 : i32
      %c2_i32_418 = arith.constant 2 : i32
      llvm.call_intrinsic "llvm.amdgcn.sched.group.barrier"(%c512_i32_417, %c2_i32_418, %c0_i32_414) : (i32, i32, i32) -> ()
      %c8_i32_419 = arith.constant 8 : i32
      %c2_i32_420 = arith.constant 2 : i32
      llvm.call_intrinsic "llvm.amdgcn.sched.group.barrier"(%c8_i32_419, %c2_i32_420, %c0_i32_414) : (i32, i32, i32) -> ()
      %c7_i32_421 = arith.constant 7 : i32
      llvm.call_intrinsic "llvm.amdgcn.sched.barrier"(%c7_i32_421) : (i32) -> ()
      amdgpu.lds_barrier
      amdgpu.lds_barrier
      %c16_422 = arith.constant 16 : index
      %c32_423 = arith.constant 32 : index
      %c1_424 = arith.constant 1 : index
      %c64_425 = arith.constant 64 : index
      %468 = arith.divsi %thread_id_x, %c64_425 : index
      %469 = arith.muli %468, %c32_423 : index
      %c64_426 = arith.constant 64 : index
      %c1_427 = arith.constant 1 : index
      %c160_428 = arith.constant 160 : index
      %c0_429 = arith.constant 0 : index
      %470 = arith.divsi %c0_429, %c160_428 : index
      %471 = arith.muli %470, %c64_426 : index
      %c16_430 = arith.constant 16 : index
      %472 = arith.remsi %thread_id_x, %c16_430 : index
      %473 = arith.addi %472, %471 : index
      %474 = arith.addi %473, %469 : index
      %475 = arith.addi %474, %c16_422 : index
      %c16_431 = arith.constant 16 : index
      %c4_432 = arith.constant 4 : index
      %c1_433 = arith.constant 1 : index
      %c16_434 = arith.constant 16 : index
      %c64_435 = arith.constant 64 : index
      %476 = arith.remsi %thread_id_x, %c64_435 : index
      %477 = arith.divsi %476, %c16_434 : index
      %478 = arith.muli %477, %c4_432 : index
      %479 = arith.addi %478, %c16_431 : index
      %480 = vector.load %alloc_0[%475, %479] : memref<64x36xf16, #gpu.address_space<workgroup>>, vector<4xf16>
      %c16_436 = arith.constant 16 : index
      %c32_437 = arith.constant 32 : index
      %481 = arith.muli %thread_id_y, %c32_437 : index
      %c64_438 = arith.constant 64 : index
      %c0_439 = arith.constant 0 : index
      %c160_440 = arith.constant 160 : index
      %482 = arith.remsi %c0_439, %c160_440 : index
      %483 = arith.muli %482, %c64_438 : index
      %c16_441 = arith.constant 16 : index
      %484 = arith.remsi %thread_id_x, %c16_441 : index
      %485 = arith.addi %484, %483 : index
      %486 = arith.addi %485, %481 : index
      %487 = arith.addi %486, %c16_436 : index
      %c16_442 = arith.constant 16 : index
      %c4_443 = arith.constant 4 : index
      %c1_444 = arith.constant 1 : index
      %c16_445 = arith.constant 16 : index
      %c64_446 = arith.constant 64 : index
      %488 = arith.remsi %thread_id_x, %c64_446 : index
      %489 = arith.divsi %488, %c16_445 : index
      %490 = arith.muli %489, %c4_443 : index
      %491 = arith.addi %490, %c16_442 : index
      %492 = vector.load %alloc[%487, %491] : memref<64x36xf16, #gpu.address_space<workgroup>>, vector<4xf16>
      %493 = amdgpu.mfma %arg12 * %arg11 + %arg6 {blocks = 1 : i32, k = 16 : i32, m = 16 : i32, n = 16 : i32} blgp =  none : vector<4xf16>, vector<4xf16>, vector<4xf32>
      %494 = amdgpu.mfma %arg8 * %arg10 + %arg15 {blocks = 1 : i32, k = 16 : i32, m = 16 : i32, n = 16 : i32} blgp =  none : vector<4xf16>, vector<4xf16>, vector<4xf32>
      %c0_i32_447 = arith.constant 0 : i32
      %c256_i32_448 = arith.constant 256 : i32
      %c2_i32_449 = arith.constant 2 : i32
      llvm.call_intrinsic "llvm.amdgcn.sched.group.barrier"(%c256_i32_448, %c2_i32_449, %c0_i32_447) : (i32, i32, i32) -> ()
      %c8_i32_450 = arith.constant 8 : i32
      %c2_i32_451 = arith.constant 2 : i32
      llvm.call_intrinsic "llvm.amdgcn.sched.group.barrier"(%c8_i32_450, %c2_i32_451, %c0_i32_447) : (i32, i32, i32) -> ()
      %c7_i32_452 = arith.constant 7 : i32
      llvm.call_intrinsic "llvm.amdgcn.sched.barrier"(%c7_i32_452) : (i32) -> ()
      %c16_453 = arith.constant 16 : index
      %c32_454 = arith.constant 32 : index
      %c1_455 = arith.constant 1 : index
      %c64_456 = arith.constant 64 : index
      %495 = arith.divsi %thread_id_x, %c64_456 : index
      %496 = arith.muli %495, %c32_454 : index
      %c64_457 = arith.constant 64 : index
      %c1_458 = arith.constant 1 : index
      %c160_459 = arith.constant 160 : index
      %c0_460 = arith.constant 0 : index
      %497 = arith.divsi %c0_460, %c160_459 : index
      %498 = arith.muli %497, %c64_457 : index
      %c16_461 = arith.constant 16 : index
      %499 = arith.remsi %thread_id_x, %c16_461 : index
      %500 = arith.addi %499, %498 : index
      %501 = arith.addi %500, %496 : index
      %502 = arith.addi %501, %c16_453 : index
      %c4_462 = arith.constant 4 : index
      %c1_463 = arith.constant 1 : index
      %c16_464 = arith.constant 16 : index
      %c64_465 = arith.constant 64 : index
      %503 = arith.remsi %thread_id_x, %c64_465 : index
      %504 = arith.divsi %503, %c16_464 : index
      %505 = arith.muli %504, %c4_462 : index
      %506 = vector.load %alloc_0[%502, %505] : memref<64x36xf16, #gpu.address_space<workgroup>>, vector<4xf16>
      %c16_466 = arith.constant 16 : index
      %c32_467 = arith.constant 32 : index
      %507 = arith.muli %thread_id_y, %c32_467 : index
      %c64_468 = arith.constant 64 : index
      %c0_469 = arith.constant 0 : index
      %c160_470 = arith.constant 160 : index
      %508 = arith.remsi %c0_469, %c160_470 : index
      %509 = arith.muli %508, %c64_468 : index
      %c16_471 = arith.constant 16 : index
      %510 = arith.remsi %thread_id_x, %c16_471 : index
      %511 = arith.addi %510, %509 : index
      %512 = arith.addi %511, %507 : index
      %513 = arith.addi %512, %c16_466 : index
      %c4_472 = arith.constant 4 : index
      %c1_473 = arith.constant 1 : index
      %c16_474 = arith.constant 16 : index
      %c64_475 = arith.constant 64 : index
      %514 = arith.remsi %thread_id_x, %c64_475 : index
      %515 = arith.divsi %514, %c16_474 : index
      %516 = arith.muli %515, %c4_472 : index
      %517 = vector.load %alloc[%513, %516] : memref<64x36xf16, #gpu.address_space<workgroup>>, vector<4xf16>
      %518 = amdgpu.mfma %arg9 * %arg14 + %467 {blocks = 1 : i32, k = 16 : i32, m = 16 : i32, n = 16 : i32} blgp =  none : vector<4xf16>, vector<4xf16>, vector<4xf32>
      %c0_i32_476 = arith.constant 0 : i32
      %c256_i32_477 = arith.constant 256 : i32
      %c2_i32_478 = arith.constant 2 : i32
      llvm.call_intrinsic "llvm.amdgcn.sched.group.barrier"(%c256_i32_477, %c2_i32_478, %c0_i32_476) : (i32, i32, i32) -> ()
      %c8_i32_479 = arith.constant 8 : i32
      %c1_i32_480 = arith.constant 1 : i32
      llvm.call_intrinsic "llvm.amdgcn.sched.group.barrier"(%c8_i32_479, %c1_i32_480, %c0_i32_476) : (i32, i32, i32) -> ()
      %c7_i32_481 = arith.constant 7 : i32
      llvm.call_intrinsic "llvm.amdgcn.sched.barrier"(%c7_i32_481) : (i32) -> ()
      %519 = amdgpu.mfma %506 * %517 + %466 {blocks = 1 : i32, k = 16 : i32, m = 16 : i32, n = 16 : i32} blgp =  none : vector<4xf16>, vector<4xf16>, vector<4xf32>
      %c32_482 = arith.constant 32 : index
      %520 = arith.muli %thread_id_y, %c32_482 : index
      %c64_483 = arith.constant 64 : index
      %c0_484 = arith.constant 0 : index
      %c160_485 = arith.constant 160 : index
      %521 = arith.remsi %c0_484, %c160_485 : index
      %522 = arith.muli %521, %c64_483 : index
      %c16_486 = arith.constant 16 : index
      %523 = arith.remsi %thread_id_x, %c16_486 : index
      %524 = arith.addi %523, %522 : index
      %525 = arith.addi %524, %520 : index
      %c16_487 = arith.constant 16 : index
      %c4_488 = arith.constant 4 : index
      %c1_489 = arith.constant 1 : index
      %c16_490 = arith.constant 16 : index
      %c64_491 = arith.constant 64 : index
      %526 = arith.remsi %thread_id_x, %c64_491 : index
      %527 = arith.divsi %526, %c16_490 : index
      %528 = arith.muli %527, %c4_488 : index
      %529 = arith.addi %528, %c16_487 : index
      %530 = vector.load %alloc[%525, %529] : memref<64x36xf16, #gpu.address_space<workgroup>>, vector<4xf16>
      %c32_492 = arith.constant 32 : index
      %531 = arith.muli %thread_id_y, %c32_492 : index
      %c64_493 = arith.constant 64 : index
      %c0_494 = arith.constant 0 : index
      %c160_495 = arith.constant 160 : index
      %532 = arith.remsi %c0_494, %c160_495 : index
      %533 = arith.muli %532, %c64_493 : index
      %c16_496 = arith.constant 16 : index
      %534 = arith.remsi %thread_id_x, %c16_496 : index
      %535 = arith.addi %534, %533 : index
      %536 = arith.addi %535, %531 : index
      %c4_497 = arith.constant 4 : index
      %c1_498 = arith.constant 1 : index
      %c16_499 = arith.constant 16 : index
      %c64_500 = arith.constant 64 : index
      %537 = arith.remsi %thread_id_x, %c64_500 : index
      %538 = arith.divsi %537, %c16_499 : index
      %539 = arith.muli %538, %c4_497 : index
      %540 = vector.load %alloc[%536, %539] : memref<64x36xf16, #gpu.address_space<workgroup>>, vector<4xf16>
      %541 = amdgpu.mfma %arg9 * %arg11 + %494 {blocks = 1 : i32, k = 16 : i32, m = 16 : i32, n = 16 : i32} blgp =  none : vector<4xf16>, vector<4xf16>, vector<4xf32>
      %c0_i32_501 = arith.constant 0 : i32
      %c8_i32_502 = arith.constant 8 : i32
      %c2_i32_503 = arith.constant 2 : i32
      llvm.call_intrinsic "llvm.amdgcn.sched.group.barrier"(%c8_i32_502, %c2_i32_503, %c0_i32_501) : (i32, i32, i32) -> ()
      %c256_i32_504 = arith.constant 256 : i32
      %c2_i32_505 = arith.constant 2 : i32
      llvm.call_intrinsic "llvm.amdgcn.sched.group.barrier"(%c256_i32_504, %c2_i32_505, %c0_i32_501) : (i32, i32, i32) -> ()
      %c7_i32_506 = arith.constant 7 : i32
      llvm.call_intrinsic "llvm.amdgcn.sched.barrier"(%c7_i32_506) : (i32) -> ()
      %542 = amdgpu.mfma %506 * %540 + %493 {blocks = 1 : i32, k = 16 : i32, m = 16 : i32, n = 16 : i32} blgp =  none : vector<4xf16>, vector<4xf16>, vector<4xf32>
      %c32_507 = arith.constant 32 : index
      %c1_508 = arith.constant 1 : index
      %c64_509 = arith.constant 64 : index
      %543 = arith.divsi %thread_id_x, %c64_509 : index
      %544 = arith.muli %543, %c32_507 : index
      %c64_510 = arith.constant 64 : index
      %c1_511 = arith.constant 1 : index
      %c160_512 = arith.constant 160 : index
      %c0_513 = arith.constant 0 : index
      %545 = arith.divsi %c0_513, %c160_512 : index
      %546 = arith.muli %545, %c64_510 : index
      %c16_514 = arith.constant 16 : index
      %547 = arith.remsi %thread_id_x, %c16_514 : index
      %548 = arith.addi %547, %546 : index
      %549 = arith.addi %548, %544 : index
      %c16_515 = arith.constant 16 : index
      %c4_516 = arith.constant 4 : index
      %c1_517 = arith.constant 1 : index
      %c16_518 = arith.constant 16 : index
      %c64_519 = arith.constant 64 : index
      %550 = arith.remsi %thread_id_x, %c64_519 : index
      %551 = arith.divsi %550, %c16_518 : index
      %552 = arith.muli %551, %c4_516 : index
      %553 = arith.addi %552, %c16_515 : index
      %554 = vector.load %alloc_0[%549, %553] : memref<64x36xf16, #gpu.address_space<workgroup>>, vector<4xf16>
      %c32_520 = arith.constant 32 : index
      %c1_521 = arith.constant 1 : index
      %c64_522 = arith.constant 64 : index
      %555 = arith.divsi %thread_id_x, %c64_522 : index
      %556 = arith.muli %555, %c32_520 : index
      %c64_523 = arith.constant 64 : index
      %c1_524 = arith.constant 1 : index
      %c160_525 = arith.constant 160 : index
      %c0_526 = arith.constant 0 : index
      %557 = arith.divsi %c0_526, %c160_525 : index
      %558 = arith.muli %557, %c64_523 : index
      %c16_527 = arith.constant 16 : index
      %559 = arith.remsi %thread_id_x, %c16_527 : index
      %560 = arith.addi %559, %558 : index
      %561 = arith.addi %560, %556 : index
      %c4_528 = arith.constant 4 : index
      %c1_529 = arith.constant 1 : index
      %c16_530 = arith.constant 16 : index
      %c64_531 = arith.constant 64 : index
      %562 = arith.remsi %thread_id_x, %c64_531 : index
      %563 = arith.divsi %562, %c16_530 : index
      %564 = arith.muli %563, %c4_528 : index
      %565 = vector.load %alloc_0[%561, %564] : memref<64x36xf16, #gpu.address_space<workgroup>>, vector<4xf16>
      %c0_i32_532 = arith.constant 0 : i32
      %c8_i32_533 = arith.constant 8 : i32
      %c1_i32_534 = arith.constant 1 : i32
      llvm.call_intrinsic "llvm.amdgcn.sched.group.barrier"(%c8_i32_533, %c1_i32_534, %c0_i32_532) : (i32, i32, i32) -> ()
      %c256_i32_535 = arith.constant 256 : i32
      %c2_i32_536 = arith.constant 2 : i32
      llvm.call_intrinsic "llvm.amdgcn.sched.group.barrier"(%c256_i32_535, %c2_i32_536, %c0_i32_532) : (i32, i32, i32) -> ()
      %c7_i32_537 = arith.constant 7 : i32
      llvm.call_intrinsic "llvm.amdgcn.sched.barrier"(%c7_i32_537) : (i32) -> ()
      scf.yield %430, %445, %542, %519, %565, %554, %540, %530, %480, %517, %492, %541, %518, %493, %466 : vector<8xf16>, vector<8xf16>, vector<4xf32>, vector<4xf32>, vector<4xf16>, vector<4xf16>, vector<4xf16>, vector<4xf16>, vector<4xf16>, vector<4xf16>, vector<4xf16>, vector<4xf32>, vector<4xf32>, vector<4xf32>, vector<4xf32>
    }
    %c40_178 = arith.constant 40 : index
    %177 = amdgpu.mfma %176#8 * %176#10 + %176#3 {blocks = 1 : i32, k = 16 : i32, m = 16 : i32, n = 16 : i32} blgp =  none : vector<4xf16>, vector<4xf16>, vector<4xf32>
    %178 = amdgpu.mfma %176#4 * %176#9 + %176#12 {blocks = 1 : i32, k = 16 : i32, m = 16 : i32, n = 16 : i32} blgp =  none : vector<4xf16>, vector<4xf16>, vector<4xf32>
    %179 = amdgpu.mfma %176#8 * %176#7 + %176#2 {blocks = 1 : i32, k = 16 : i32, m = 16 : i32, n = 16 : i32} blgp =  none : vector<4xf16>, vector<4xf16>, vector<4xf32>
    %180 = amdgpu.mfma %176#4 * %176#6 + %176#11 {blocks = 1 : i32, k = 16 : i32, m = 16 : i32, n = 16 : i32} blgp =  none : vector<4xf16>, vector<4xf16>, vector<4xf32>
    %181 = amdgpu.mfma %176#5 * %176#10 + %178 {blocks = 1 : i32, k = 16 : i32, m = 16 : i32, n = 16 : i32} blgp =  none : vector<4xf16>, vector<4xf16>, vector<4xf32>
    %182 = amdgpu.mfma %176#5 * %176#7 + %180 {blocks = 1 : i32, k = 16 : i32, m = 16 : i32, n = 16 : i32} blgp =  none : vector<4xf16>, vector<4xf16>, vector<4xf32>
    amdgpu.lds_barrier
    %c32_179 = arith.constant 32 : index
    %183 = arith.muli %thread_id_y, %c32_179 : index
    %c64_180 = arith.constant 64 : index
    %184 = arith.muli %thread_id_z, %c64_180 : index
    %c64_181 = arith.constant 64 : index
    %c1_182 = arith.constant 1 : index
    %c160_183 = arith.constant 160 : index
    %c0_184 = arith.constant 0 : index
    %185 = arith.divsi %c0_184, %c160_183 : index
    %186 = arith.muli %185, %c64_181 : index
    %c1_185 = arith.constant 1 : index
    %c4_186 = arith.constant 4 : index
    %187 = arith.divsi %thread_id_x, %c4_186 : index
    %188 = arith.addi %187, %186 : index
    %189 = arith.addi %188, %184 : index
    %190 = arith.addi %189, %183 : index
    %c8_187 = arith.constant 8 : index
    %c4_188 = arith.constant 4 : index
    %191 = arith.remsi %thread_id_x, %c4_188 : index
    %192 = arith.muli %191, %c8_187 : index
    vector.store %176#0, %alloc_0[%190, %192] : memref<64x36xf16, #gpu.address_space<workgroup>>, vector<8xf16>
    %c32_189 = arith.constant 32 : index
    %193 = arith.muli %thread_id_y, %c32_189 : index
    %c64_190 = arith.constant 64 : index
    %194 = arith.muli %thread_id_z, %c64_190 : index
    %c64_191 = arith.constant 64 : index
    %c0_192 = arith.constant 0 : index
    %c160_193 = arith.constant 160 : index
    %195 = arith.remsi %c0_192, %c160_193 : index
    %196 = arith.muli %195, %c64_191 : index
    %c1_194 = arith.constant 1 : index
    %c4_195 = arith.constant 4 : index
    %197 = arith.divsi %thread_id_x, %c4_195 : index
    %198 = arith.addi %197, %196 : index
    %199 = arith.addi %198, %194 : index
    %200 = arith.addi %199, %193 : index
    %c8_196 = arith.constant 8 : index
    %c4_197 = arith.constant 4 : index
    %201 = arith.remsi %thread_id_x, %c4_197 : index
    %202 = arith.muli %201, %c8_196 : index
    vector.store %176#1, %alloc[%200, %202] : memref<64x36xf16, #gpu.address_space<workgroup>>, vector<8xf16>
    amdgpu.lds_barrier
    amdgpu.lds_barrier
    %c16_198 = arith.constant 16 : index
    %c32_199 = arith.constant 32 : index
    %c1_200 = arith.constant 1 : index
    %c64_201 = arith.constant 64 : index
    %203 = arith.divsi %thread_id_x, %c64_201 : index
    %204 = arith.muli %203, %c32_199 : index
    %c64_202 = arith.constant 64 : index
    %c1_203 = arith.constant 1 : index
    %c160_204 = arith.constant 160 : index
    %c0_205 = arith.constant 0 : index
    %205 = arith.divsi %c0_205, %c160_204 : index
    %206 = arith.muli %205, %c64_202 : index
    %c16_206 = arith.constant 16 : index
    %207 = arith.remsi %thread_id_x, %c16_206 : index
    %208 = arith.addi %207, %206 : index
    %209 = arith.addi %208, %204 : index
    %210 = arith.addi %209, %c16_198 : index
    %c16_207 = arith.constant 16 : index
    %c4_208 = arith.constant 4 : index
    %c1_209 = arith.constant 1 : index
    %c16_210 = arith.constant 16 : index
    %c64_211 = arith.constant 64 : index
    %211 = arith.remsi %thread_id_x, %c64_211 : index
    %212 = arith.divsi %211, %c16_210 : index
    %213 = arith.muli %212, %c4_208 : index
    %214 = arith.addi %213, %c16_207 : index
    %215 = vector.load %alloc_0[%210, %214] : memref<64x36xf16, #gpu.address_space<workgroup>>, vector<4xf16>
    %c16_212 = arith.constant 16 : index
    %c32_213 = arith.constant 32 : index
    %216 = arith.muli %thread_id_y, %c32_213 : index
    %c64_214 = arith.constant 64 : index
    %c0_215 = arith.constant 0 : index
    %c160_216 = arith.constant 160 : index
    %217 = arith.remsi %c0_215, %c160_216 : index
    %218 = arith.muli %217, %c64_214 : index
    %c16_217 = arith.constant 16 : index
    %219 = arith.remsi %thread_id_x, %c16_217 : index
    %220 = arith.addi %219, %218 : index
    %221 = arith.addi %220, %216 : index
    %222 = arith.addi %221, %c16_212 : index
    %c16_218 = arith.constant 16 : index
    %c4_219 = arith.constant 4 : index
    %c1_220 = arith.constant 1 : index
    %c16_221 = arith.constant 16 : index
    %c64_222 = arith.constant 64 : index
    %223 = arith.remsi %thread_id_x, %c64_222 : index
    %224 = arith.divsi %223, %c16_221 : index
    %225 = arith.muli %224, %c4_219 : index
    %226 = arith.addi %225, %c16_218 : index
    %227 = vector.load %alloc[%222, %226] : memref<64x36xf16, #gpu.address_space<workgroup>>, vector<4xf16>
    %c16_223 = arith.constant 16 : index
    %c32_224 = arith.constant 32 : index
    %c1_225 = arith.constant 1 : index
    %c64_226 = arith.constant 64 : index
    %228 = arith.divsi %thread_id_x, %c64_226 : index
    %229 = arith.muli %228, %c32_224 : index
    %c64_227 = arith.constant 64 : index
    %c1_228 = arith.constant 1 : index
    %c160_229 = arith.constant 160 : index
    %c0_230 = arith.constant 0 : index
    %230 = arith.divsi %c0_230, %c160_229 : index
    %231 = arith.muli %230, %c64_227 : index
    %c16_231 = arith.constant 16 : index
    %232 = arith.remsi %thread_id_x, %c16_231 : index
    %233 = arith.addi %232, %231 : index
    %234 = arith.addi %233, %229 : index
    %235 = arith.addi %234, %c16_223 : index
    %c4_232 = arith.constant 4 : index
    %c1_233 = arith.constant 1 : index
    %c16_234 = arith.constant 16 : index
    %c64_235 = arith.constant 64 : index
    %236 = arith.remsi %thread_id_x, %c64_235 : index
    %237 = arith.divsi %236, %c16_234 : index
    %238 = arith.muli %237, %c4_232 : index
    %239 = vector.load %alloc_0[%235, %238] : memref<64x36xf16, #gpu.address_space<workgroup>>, vector<4xf16>
    %c16_236 = arith.constant 16 : index
    %c32_237 = arith.constant 32 : index
    %240 = arith.muli %thread_id_y, %c32_237 : index
    %c64_238 = arith.constant 64 : index
    %c0_239 = arith.constant 0 : index
    %c160_240 = arith.constant 160 : index
    %241 = arith.remsi %c0_239, %c160_240 : index
    %242 = arith.muli %241, %c64_238 : index
    %c16_241 = arith.constant 16 : index
    %243 = arith.remsi %thread_id_x, %c16_241 : index
    %244 = arith.addi %243, %242 : index
    %245 = arith.addi %244, %240 : index
    %246 = arith.addi %245, %c16_236 : index
    %c4_242 = arith.constant 4 : index
    %c1_243 = arith.constant 1 : index
    %c16_244 = arith.constant 16 : index
    %c64_245 = arith.constant 64 : index
    %247 = arith.remsi %thread_id_x, %c64_245 : index
    %248 = arith.divsi %247, %c16_244 : index
    %249 = arith.muli %248, %c4_242 : index
    %250 = vector.load %alloc[%246, %249] : memref<64x36xf16, #gpu.address_space<workgroup>>, vector<4xf16>
    %251 = amdgpu.mfma %239 * %250 + %177 {blocks = 1 : i32, k = 16 : i32, m = 16 : i32, n = 16 : i32} blgp =  none : vector<4xf16>, vector<4xf16>, vector<4xf32>
    %c32_246 = arith.constant 32 : index
    %252 = arith.muli %thread_id_y, %c32_246 : index
    %c64_247 = arith.constant 64 : index
    %c0_248 = arith.constant 0 : index
    %c160_249 = arith.constant 160 : index
    %253 = arith.remsi %c0_248, %c160_249 : index
    %254 = arith.muli %253, %c64_247 : index
    %c16_250 = arith.constant 16 : index
    %255 = arith.remsi %thread_id_x, %c16_250 : index
    %256 = arith.addi %255, %254 : index
    %257 = arith.addi %256, %252 : index
    %c16_251 = arith.constant 16 : index
    %c4_252 = arith.constant 4 : index
    %c1_253 = arith.constant 1 : index
    %c16_254 = arith.constant 16 : index
    %c64_255 = arith.constant 64 : index
    %258 = arith.remsi %thread_id_x, %c64_255 : index
    %259 = arith.divsi %258, %c16_254 : index
    %260 = arith.muli %259, %c4_252 : index
    %261 = arith.addi %260, %c16_251 : index
    %262 = vector.load %alloc[%257, %261] : memref<64x36xf16, #gpu.address_space<workgroup>>, vector<4xf16>
    %c32_256 = arith.constant 32 : index
    %263 = arith.muli %thread_id_y, %c32_256 : index
    %c64_257 = arith.constant 64 : index
    %c0_258 = arith.constant 0 : index
    %c160_259 = arith.constant 160 : index
    %264 = arith.remsi %c0_258, %c160_259 : index
    %265 = arith.muli %264, %c64_257 : index
    %c16_260 = arith.constant 16 : index
    %266 = arith.remsi %thread_id_x, %c16_260 : index
    %267 = arith.addi %266, %265 : index
    %268 = arith.addi %267, %263 : index
    %c4_261 = arith.constant 4 : index
    %c1_262 = arith.constant 1 : index
    %c16_263 = arith.constant 16 : index
    %c64_264 = arith.constant 64 : index
    %269 = arith.remsi %thread_id_x, %c64_264 : index
    %270 = arith.divsi %269, %c16_263 : index
    %271 = arith.muli %270, %c4_261 : index
    %272 = vector.load %alloc[%268, %271] : memref<64x36xf16, #gpu.address_space<workgroup>>, vector<4xf16>
    %273 = amdgpu.mfma %239 * %272 + %179 {blocks = 1 : i32, k = 16 : i32, m = 16 : i32, n = 16 : i32} blgp =  none : vector<4xf16>, vector<4xf16>, vector<4xf32>
    %c32_265 = arith.constant 32 : index
    %c1_266 = arith.constant 1 : index
    %c64_267 = arith.constant 64 : index
    %274 = arith.divsi %thread_id_x, %c64_267 : index
    %275 = arith.muli %274, %c32_265 : index
    %c64_268 = arith.constant 64 : index
    %c1_269 = arith.constant 1 : index
    %c160_270 = arith.constant 160 : index
    %c0_271 = arith.constant 0 : index
    %276 = arith.divsi %c0_271, %c160_270 : index
    %277 = arith.muli %276, %c64_268 : index
    %c16_272 = arith.constant 16 : index
    %278 = arith.remsi %thread_id_x, %c16_272 : index
    %279 = arith.addi %278, %277 : index
    %280 = arith.addi %279, %275 : index
    %c16_273 = arith.constant 16 : index
    %c4_274 = arith.constant 4 : index
    %c1_275 = arith.constant 1 : index
    %c16_276 = arith.constant 16 : index
    %c64_277 = arith.constant 64 : index
    %281 = arith.remsi %thread_id_x, %c64_277 : index
    %282 = arith.divsi %281, %c16_276 : index
    %283 = arith.muli %282, %c4_274 : index
    %284 = arith.addi %283, %c16_273 : index
    %285 = vector.load %alloc_0[%280, %284] : memref<64x36xf16, #gpu.address_space<workgroup>>, vector<4xf16>
    %c32_278 = arith.constant 32 : index
    %c1_279 = arith.constant 1 : index
    %c64_280 = arith.constant 64 : index
    %286 = arith.divsi %thread_id_x, %c64_280 : index
    %287 = arith.muli %286, %c32_278 : index
    %c64_281 = arith.constant 64 : index
    %c1_282 = arith.constant 1 : index
    %c160_283 = arith.constant 160 : index
    %c0_284 = arith.constant 0 : index
    %288 = arith.divsi %c0_284, %c160_283 : index
    %289 = arith.muli %288, %c64_281 : index
    %c16_285 = arith.constant 16 : index
    %290 = arith.remsi %thread_id_x, %c16_285 : index
    %291 = arith.addi %290, %289 : index
    %292 = arith.addi %291, %287 : index
    %c4_286 = arith.constant 4 : index
    %c1_287 = arith.constant 1 : index
    %c16_288 = arith.constant 16 : index
    %c64_289 = arith.constant 64 : index
    %293 = arith.remsi %thread_id_x, %c64_289 : index
    %294 = arith.divsi %293, %c16_288 : index
    %295 = arith.muli %294, %c4_286 : index
    %296 = vector.load %alloc_0[%292, %295] : memref<64x36xf16, #gpu.address_space<workgroup>>, vector<4xf16>
    %297 = amdgpu.mfma %215 * %227 + %251 {blocks = 1 : i32, k = 16 : i32, m = 16 : i32, n = 16 : i32} blgp =  none : vector<4xf16>, vector<4xf16>, vector<4xf32>
    %c0_290 = arith.constant 0 : index
    %c16_291 = arith.constant 16 : index
    %c4_292 = arith.constant 4 : index
    %c1_293 = arith.constant 1 : index
    %c16_294 = arith.constant 16 : index
    %c64_295 = arith.constant 64 : index
    %299 = arith.remsi %thread_id_x, %c64_295 : index
    %300 = arith.divsi %299, %c16_294 : index
    %301 = arith.muli %300, %c4_292 : index
    %c32_296 = arith.constant 32 : index
    %c1_297 = arith.constant 1 : index
    %c64_298 = arith.constant 64 : index
    %302 = arith.divsi %thread_id_x, %c64_298 : index
    %303 = arith.muli %302, %c32_296 : index
    %c64_299 = arith.constant 64 : index
    %c1_300 = arith.constant 1 : index
    %c160_301 = arith.constant 160 : index
    %c32_302 = arith.constant 32 : index
    %304 = arith.muli %workgroup_id_1, %c32_302 : index
    %305 = arith.addi %304, %workgroup_id_0 : index
    %306 = arith.divsi %305, %c160_301 : index
    %307 = arith.muli %306, %c64_299 : index
    %308 = arith.addi %307, %303 : index
    %309 = arith.addi %308, %301 : index
    %310 = arith.addi %309, %c16_291 : index
    %c16_303 = arith.constant 16 : index
    %c32_304 = arith.constant 32 : index
    %311 = arith.muli %thread_id_y, %c32_304 : index
    %c64_305 = arith.constant 64 : index
    %c32_306 = arith.constant 32 : index
    %312 = arith.muli %workgroup_id_1, %c32_306 : index
    %313 = arith.addi %312, %workgroup_id_0 : index
    %c160_307 = arith.constant 160 : index
    %314 = arith.remsi %313, %c160_307 : index
    %315 = arith.muli %314, %c64_305 : index
    %c16_308 = arith.constant 16 : index
    %316 = arith.remsi %thread_id_x, %c16_308 : index
    %317 = arith.addi %316, %315 : index
    %318 = arith.addi %317, %311 : index
    %319 = arith.addi %318, %c16_303 : index
    %c0_309 = arith.constant 0 : index
    %320 = vector.extract_strided_slice %297 {offsets = [0], sizes = [1], strides = [1]} : vector<4xf32> to vector<1xf32>
    %321 = arith.addi %310, %c0_309 : index
    vector.store %320, %298[%321, %319] : memref<2048x10240xf32, strided<[10240, 1], offset: ?>>, vector<1xf32>
    %c1_310 = arith.constant 1 : index
    %322 = vector.extract_strided_slice %297 {offsets = [1], sizes = [1], strides = [1]} : vector<4xf32> to vector<1xf32>
    %323 = arith.addi %310, %c1_310 : index
    vector.store %322, %298[%323, %319] : memref<2048x10240xf32, strided<[10240, 1], offset: ?>>, vector<1xf32>
    %c2_311 = arith.constant 2 : index
    %324 = vector.extract_strided_slice %297 {offsets = [2], sizes = [1], strides = [1]} : vector<4xf32> to vector<1xf32>
    %325 = arith.addi %310, %c2_311 : index
    vector.store %324, %298[%325, %319] : memref<2048x10240xf32, strided<[10240, 1], offset: ?>>, vector<1xf32>
    %c3 = arith.constant 3 : index
    %326 = vector.extract_strided_slice %297 {offsets = [3], sizes = [1], strides = [1]} : vector<4xf32> to vector<1xf32>
    %327 = arith.addi %310, %c3 : index
    vector.store %326, %298[%327, %319] : memref<2048x10240xf32, strided<[10240, 1], offset: ?>>, vector<1xf32>
    %328 = amdgpu.mfma %296 * %250 + %181 {blocks = 1 : i32, k = 16 : i32, m = 16 : i32, n = 16 : i32} blgp =  none : vector<4xf16>, vector<4xf16>, vector<4xf32>
    %329 = amdgpu.mfma %215 * %262 + %273 {blocks = 1 : i32, k = 16 : i32, m = 16 : i32, n = 16 : i32} blgp =  none : vector<4xf16>, vector<4xf16>, vector<4xf32>
    %c16_312 = arith.constant 16 : index
    %c4_313 = arith.constant 4 : index
    %c1_314 = arith.constant 1 : index
    %c16_315 = arith.constant 16 : index
    %c64_316 = arith.constant 64 : index
    %330 = arith.remsi %thread_id_x, %c64_316 : index
    %331 = arith.divsi %330, %c16_315 : index
    %332 = arith.muli %331, %c4_313 : index
    %c32_317 = arith.constant 32 : index
    %c1_318 = arith.constant 1 : index
    %c64_319 = arith.constant 64 : index
    %333 = arith.divsi %thread_id_x, %c64_319 : index
    %334 = arith.muli %333, %c32_317 : index
    %c64_320 = arith.constant 64 : index
    %c1_321 = arith.constant 1 : index
    %c160_322 = arith.constant 160 : index
    %c32_323 = arith.constant 32 : index
    %335 = arith.muli %workgroup_id_1, %c32_323 : index
    %336 = arith.addi %335, %workgroup_id_0 : index
    %337 = arith.divsi %336, %c160_322 : index
    %338 = arith.muli %337, %c64_320 : index
    %339 = arith.addi %338, %334 : index
    %340 = arith.addi %339, %332 : index
    %341 = arith.addi %340, %c16_312 : index
    %c32_324 = arith.constant 32 : index
    %342 = arith.muli %thread_id_y, %c32_324 : index
    %c64_325 = arith.constant 64 : index
    %c32_326 = arith.constant 32 : index
    %343 = arith.muli %workgroup_id_1, %c32_326 : index
    %344 = arith.addi %343, %workgroup_id_0 : index
    %c160_327 = arith.constant 160 : index
    %345 = arith.remsi %344, %c160_327 : index
    %346 = arith.muli %345, %c64_325 : index
    %c16_328 = arith.constant 16 : index
    %347 = arith.remsi %thread_id_x, %c16_328 : index
    %348 = arith.addi %347, %346 : index
    %349 = arith.addi %348, %342 : index
    %c0_329 = arith.constant 0 : index
    %350 = vector.extract_strided_slice %329 {offsets = [0], sizes = [1], strides = [1]} : vector<4xf32> to vector<1xf32>
    %351 = arith.addi %341, %c0_329 : index
    vector.store %350, %298[%351, %349] : memref<2048x10240xf32, strided<[10240, 1], offset: ?>>, vector<1xf32>
    %c1_330 = arith.constant 1 : index
    %352 = vector.extract_strided_slice %329 {offsets = [1], sizes = [1], strides = [1]} : vector<4xf32> to vector<1xf32>
    %353 = arith.addi %341, %c1_330 : index
    vector.store %352, %298[%353, %349] : memref<2048x10240xf32, strided<[10240, 1], offset: ?>>, vector<1xf32>
    %c2_331 = arith.constant 2 : index
    %354 = vector.extract_strided_slice %329 {offsets = [2], sizes = [1], strides = [1]} : vector<4xf32> to vector<1xf32>
    %355 = arith.addi %341, %c2_331 : index
    vector.store %354, %298[%355, %349] : memref<2048x10240xf32, strided<[10240, 1], offset: ?>>, vector<1xf32>
    %c3_332 = arith.constant 3 : index
    %356 = vector.extract_strided_slice %329 {offsets = [3], sizes = [1], strides = [1]} : vector<4xf32> to vector<1xf32>
    %357 = arith.addi %341, %c3_332 : index
    vector.store %356, %298[%357, %349] : memref<2048x10240xf32, strided<[10240, 1], offset: ?>>, vector<1xf32>
    %358 = amdgpu.mfma %296 * %272 + %182 {blocks = 1 : i32, k = 16 : i32, m = 16 : i32, n = 16 : i32} blgp =  none : vector<4xf16>, vector<4xf16>, vector<4xf32>
    %359 = amdgpu.mfma %285 * %227 + %328 {blocks = 1 : i32, k = 16 : i32, m = 16 : i32, n = 16 : i32} blgp =  none : vector<4xf16>, vector<4xf16>, vector<4xf32>
    %c4_333 = arith.constant 4 : index
    %c1_334 = arith.constant 1 : index
    %c16_335 = arith.constant 16 : index
    %c64_336 = arith.constant 64 : index
    %360 = arith.remsi %thread_id_x, %c64_336 : index
    %361 = arith.divsi %360, %c16_335 : index
    %362 = arith.muli %361, %c4_333 : index
    %c32_337 = arith.constant 32 : index
    %c1_338 = arith.constant 1 : index
    %c64_339 = arith.constant 64 : index
    %363 = arith.divsi %thread_id_x, %c64_339 : index
    %364 = arith.muli %363, %c32_337 : index
    %c64_340 = arith.constant 64 : index
    %c1_341 = arith.constant 1 : index
    %c160_342 = arith.constant 160 : index
    %c32_343 = arith.constant 32 : index
    %365 = arith.muli %workgroup_id_1, %c32_343 : index
    %366 = arith.addi %365, %workgroup_id_0 : index
    %367 = arith.divsi %366, %c160_342 : index
    %368 = arith.muli %367, %c64_340 : index
    %369 = arith.addi %368, %364 : index
    %370 = arith.addi %369, %362 : index
    %c16_344 = arith.constant 16 : index
    %c32_345 = arith.constant 32 : index
    %371 = arith.muli %thread_id_y, %c32_345 : index
    %c64_346 = arith.constant 64 : index
    %c32_347 = arith.constant 32 : index
    %372 = arith.muli %workgroup_id_1, %c32_347 : index
    %373 = arith.addi %372, %workgroup_id_0 : index
    %c160_348 = arith.constant 160 : index
    %374 = arith.remsi %373, %c160_348 : index
    %375 = arith.muli %374, %c64_346 : index
    %c16_349 = arith.constant 16 : index
    %376 = arith.remsi %thread_id_x, %c16_349 : index
    %377 = arith.addi %376, %375 : index
    %378 = arith.addi %377, %371 : index
    %379 = arith.addi %378, %c16_344 : index
    %c0_350 = arith.constant 0 : index
    %380 = vector.extract_strided_slice %359 {offsets = [0], sizes = [1], strides = [1]} : vector<4xf32> to vector<1xf32>
    %381 = arith.addi %370, %c0_350 : index
    vector.store %380, %298[%381, %379] : memref<2048x10240xf32, strided<[10240, 1], offset: ?>>, vector<1xf32>
    %c1_351 = arith.constant 1 : index
    %382 = vector.extract_strided_slice %359 {offsets = [1], sizes = [1], strides = [1]} : vector<4xf32> to vector<1xf32>
    %383 = arith.addi %370, %c1_351 : index
    vector.store %382, %298[%383, %379] : memref<2048x10240xf32, strided<[10240, 1], offset: ?>>, vector<1xf32>
    %c2_352 = arith.constant 2 : index
    %384 = vector.extract_strided_slice %359 {offsets = [2], sizes = [1], strides = [1]} : vector<4xf32> to vector<1xf32>
    %385 = arith.addi %370, %c2_352 : index
    vector.store %384, %298[%385, %379] : memref<2048x10240xf32, strided<[10240, 1], offset: ?>>, vector<1xf32>
    %c3_353 = arith.constant 3 : index
    %386 = vector.extract_strided_slice %359 {offsets = [3], sizes = [1], strides = [1]} : vector<4xf32> to vector<1xf32>
    %387 = arith.addi %370, %c3_353 : index
    vector.store %386, %298[%387, %379] : memref<2048x10240xf32, strided<[10240, 1], offset: ?>>, vector<1xf32>
    %388 = amdgpu.mfma %285 * %262 + %358 {blocks = 1 : i32, k = 16 : i32, m = 16 : i32, n = 16 : i32} blgp =  none : vector<4xf16>, vector<4xf16>, vector<4xf32>
    %c4_354 = arith.constant 4 : index
    %c1_355 = arith.constant 1 : index
    %c16_356 = arith.constant 16 : index
    %c64_357 = arith.constant 64 : index
    %389 = arith.remsi %thread_id_x, %c64_357 : index
    %390 = arith.divsi %389, %c16_356 : index
    %391 = arith.muli %390, %c4_354 : index
    %c32_358 = arith.constant 32 : index
    %c1_359 = arith.constant 1 : index
    %c64_360 = arith.constant 64 : index
    %392 = arith.divsi %thread_id_x, %c64_360 : index
    %393 = arith.muli %392, %c32_358 : index
    %c64_361 = arith.constant 64 : index
    %c1_362 = arith.constant 1 : index
    %c160_363 = arith.constant 160 : index
    %c32_364 = arith.constant 32 : index
    %394 = arith.muli %workgroup_id_1, %c32_364 : index
    %395 = arith.addi %394, %workgroup_id_0 : index
    %396 = arith.divsi %395, %c160_363 : index
    %397 = arith.muli %396, %c64_361 : index
    %398 = arith.addi %397, %393 : index
    %399 = arith.addi %398, %391 : index
    %c32_365 = arith.constant 32 : index
    %400 = arith.muli %thread_id_y, %c32_365 : index
    %c64_366 = arith.constant 64 : index
    %c32_367 = arith.constant 32 : index
    %401 = arith.muli %workgroup_id_1, %c32_367 : index
    %402 = arith.addi %401, %workgroup_id_0 : index
    %c160_368 = arith.constant 160 : index
    %403 = arith.remsi %402, %c160_368 : index
    %404 = arith.muli %403, %c64_366 : index
    %c16_369 = arith.constant 16 : index
    %405 = arith.remsi %thread_id_x, %c16_369 : index
    %406 = arith.addi %405, %404 : index
    %407 = arith.addi %406, %400 : index
    %c0_370 = arith.constant 0 : index
    %408 = vector.extract_strided_slice %388 {offsets = [0], sizes = [1], strides = [1]} : vector<4xf32> to vector<1xf32>
    %409 = arith.addi %399, %c0_370 : index
    vector.store %408, %298[%409, %407] : memref<2048x10240xf32, strided<[10240, 1], offset: ?>>, vector<1xf32>
    %c1_371 = arith.constant 1 : index
    %410 = vector.extract_strided_slice %388 {offsets = [1], sizes = [1], strides = [1]} : vector<4xf32> to vector<1xf32>
    %411 = arith.addi %399, %c1_371 : index
    vector.store %410, %298[%411, %407] : memref<2048x10240xf32, strided<[10240, 1], offset: ?>>, vector<1xf32>
    %c2_372 = arith.constant 2 : index
    %412 = vector.extract_strided_slice %388 {offsets = [2], sizes = [1], strides = [1]} : vector<4xf32> to vector<1xf32>
    %413 = arith.addi %399, %c2_372 : index
    vector.store %412, %298[%413, %407] : memref<2048x10240xf32, strided<[10240, 1], offset: ?>>, vector<1xf32>
    %c3_373 = arith.constant 3 : index
    %414 = vector.extract_strided_slice %388 {offsets = [3], sizes = [1], strides = [1]} : vector<4xf32> to vector<1xf32>
    %415 = arith.addi %399, %c3_373 : index
    vector.store %414, %298[%415, %407] : memref<2048x10240xf32, strided<[10240, 1], offset: ?>>, vector<1xf32>
    return
  }
}
